{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consecutive-amplifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-ac9d3cc4f90a>:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gzip\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from memory_profiler import memory_usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "laden-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Raw File Counts: {PosixPath('raw_data/tracks/tracks_1.json'): 350300, PosixPath('raw_data/tracks/tracks_0.csv'): 350300, PosixPath('raw_data/tracks/tracks_2.csv'): 350300, PosixPath('raw_data/tracks/tracks_4.csv'): 350300, PosixPath('raw_data/tracks/tracks_3.json'): 350300, PosixPath('raw_data/playlist_track/playlist_track_0.csv'): 871500, PosixPath('raw_data/playlist_track/playlist_track_2.csv'): 871500, PosixPath('raw_data/playlist_track/playlist_track_4.csv'): 871500, PosixPath('raw_data/playlist_track/playlist_track_1.json'): 871500, PosixPath('raw_data/playlist_track/playlist_track_3.json'): 871500, PosixPath('raw_data/orders/orders_3.json'): 224000, PosixPath('raw_data/orders/orders_2.csv'): 224000, PosixPath('raw_data/orders/orders_0.csv'): 224000, PosixPath('raw_data/orders/orders_4.csv'): 224000, PosixPath('raw_data/orders/orders_1.json'): 224000, PosixPath('raw_data/track_facts/track_facts_0.csv'): 350300, PosixPath('raw_data/track_facts/track_facts_2.csv'): 350300, PosixPath('raw_data/track_facts/track_facts_3.json'): 350300, PosixPath('raw_data/track_facts/track_facts_4.csv'): 350300, PosixPath('raw_data/track_facts/track_facts_1.json'): 350300}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def count_records_in_raw_files(raw_dir):\n",
    "    record_counts = {}\n",
    "\n",
    "    for subfolder in Path(raw_dir).iterdir():\n",
    "        if not subfolder.is_dir():\n",
    "            continue\n",
    "\n",
    "        for file in subfolder.iterdir():\n",
    "            count = 0\n",
    "            if file.suffix == \".csv\":\n",
    "                df = pd.read_csv(file)\n",
    "                count = len(df)\n",
    "\n",
    "            elif file.suffix == \".json\":\n",
    "                with open(file, \"r\") as f:\n",
    "                    for line in f:\n",
    "                        try:\n",
    "                            json.loads(line.strip())  # Verify line is valid JSON\n",
    "                            count += 1\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue  # Ignore bad lines\n",
    "\n",
    "            record_counts[file] = count\n",
    "\n",
    "    return record_counts\n",
    "\n",
    "# Run check\n",
    "raw_counts = count_records_in_raw_files(RAW_DIR)\n",
    "print(f\" Raw File Counts: {raw_counts}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "confidential-stamp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Comparing tracks_1.json vs. tracks_0.csv:  Identical\n",
      " Comparing tracks_0.csv vs. tracks_2.csv:  Identical\n",
      " Comparing tracks_2.csv vs. tracks_4.csv:  Identical\n",
      " Comparing tracks_4.csv vs. tracks_3.json:  Identical\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def compare_files(file1, file2):\n",
    "    \"\"\"Checks if two files contain the same records.\"\"\"\n",
    "    \n",
    "    # Load CSV if it's a CSV file\n",
    "    if file1.suffix == \".csv\":\n",
    "        df1 = pd.read_csv(file1)\n",
    "    else:\n",
    "        json_records = []\n",
    "        with open(file1, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    json_records.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        df1 = pd.DataFrame(json_records)\n",
    "\n",
    "    # Load second file\n",
    "    if file2.suffix == \".csv\":\n",
    "        df2 = pd.read_csv(file2)\n",
    "    else:\n",
    "        json_records = []\n",
    "        with open(file2, \"r\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    json_records.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "        df2 = pd.DataFrame(json_records)\n",
    "\n",
    "    identical = df1.equals(df2)\n",
    "    print(f\" Comparing {file1.name} vs. {file2.name}: {' Identical' if identical else ' Different'}\")\n",
    "    return identical\n",
    "\n",
    "# Run comparisons within the \"tracks\" dataset\n",
    "tracks_files = list(Path(\"raw_data/tracks\").iterdir())\n",
    "for i in range(len(tracks_files) - 1):\n",
    "    compare_files(tracks_files[i], tracks_files[i + 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "vital-employment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Comparing playlist_track_0.csv vs. playlist_track_2.csv:  Identical\n",
      " Comparing playlist_track_2.csv vs. playlist_track_4.csv:  Identical\n",
      " Comparing playlist_track_4.csv vs. playlist_track_1.json:  Identical\n",
      " Comparing playlist_track_1.json vs. playlist_track_3.json:  Identical\n"
     ]
    }
   ],
   "source": [
    "playlist_track_files = list(Path(\"raw_data/playlist_track\").iterdir())\n",
    "for i in range(len(playlist_track_files) - 1):\n",
    "    compare_files(playlist_track_files[i], playlist_track_files[i + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-click",
   "metadata": {},
   "source": [
    "We only process one file per subfolder because all the records are duplicated in each file as we investigated above. We're taking a composite key in a few files because a primary key is not identified for it. This is to ensure deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "engaged-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_column_names(record):\n",
    "    \"\"\"Removes table prefixes from column names.\"\"\"\n",
    "    return {k.split(\".\")[-1]: v for k, v in record.items()}  # Keeps only the last part of the column name\n",
    "\n",
    "def process_subfolder_fixed(subfolder_path, output_dir, error_log_file):\n",
    "    \"\"\"Processes **only one file per subfolder**, ensuring correct deduplication using primary keys.\"\"\"\n",
    "    subfolder_name = subfolder_path.name\n",
    "    output_file = output_dir / f\"{subfolder_name}.json.gz\"\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    processed_count = 0\n",
    "    seen_records = set()\n",
    "\n",
    "    # Pick one representative file (CSV preferred over JSON if both exist)\n",
    "    files = sorted(subfolder_path.iterdir())\n",
    "    file_to_process = next((f for f in files if f.suffix == \".csv\"), None)  # Prefer CSV\n",
    "    if file_to_process is None:\n",
    "        file_to_process = next((f for f in files if f.suffix == \".json\"), None)  # Fallback to JSON\n",
    "\n",
    "    if not file_to_process:\n",
    "        print(f\"No valid files found in {subfolder_name}, skipping...\")\n",
    "        return\n",
    "\n",
    "    print(f\" Processing only: {file_to_process.name}\")\n",
    "\n",
    "    with gzip.open(output_file, \"wt\") as gz_file, open(error_log_file, \"a\") as log:\n",
    "        if file_to_process.suffix == \".csv\":\n",
    "            for chunk in pd.read_csv(file_to_process, chunksize=1000):\n",
    "                for record in chunk.to_dict(orient=\"records\"):\n",
    "                    record = normalize_column_names(record)  # Standardize column names\n",
    "\n",
    "                    # Extract unique identifier based on dataset\n",
    "                    if subfolder_name == \"tracks\":\n",
    "                        record_key = (record[\"TrackId\"],)  # Unique per track\n",
    "                    elif subfolder_name == \"playlist_track\":\n",
    "                        record_key = (record[\"PlaylistId\"], record[\"TrackId\"])  # Unique per playlist-track pair\n",
    "                    elif subfolder_name == \"track_facts\":\n",
    "                        record_key = (record[\"genre\"], record[\"minutes\"], record[\"price\"])  # Unique per track fact\n",
    "                    elif subfolder_name == \"orders\":\n",
    "                        record_key = (\n",
    "                            record[\"invoice_date\"],\n",
    "                            record[\"UnitPrice\"],\n",
    "                            record[\"genre\"],\n",
    "                            record[\"media_type\"],\n",
    "                            record[\"minutes\"],\n",
    "                        )  # Unique per order\n",
    "                    else:\n",
    "                        record_key = tuple(sorted(record.items()))  # Fallback\n",
    "\n",
    "                    if record_key in seen_records:\n",
    "                        continue  # Skip duplicate records\n",
    "                    seen_records.add(record_key)\n",
    "\n",
    "                    gz_file.write(json.dumps(record) + \"\\n\")\n",
    "                    processed_count += 1\n",
    "\n",
    "        elif file_to_process.suffix == \".json\":\n",
    "            with open(file_to_process, \"r\") as json_file:\n",
    "                for line in json_file:\n",
    "                    try:\n",
    "                        record = json.loads(line.strip())\n",
    "                        record = normalize_column_names(record)  # Standardize column names\n",
    "\n",
    "                        if subfolder_name == \"tracks\":\n",
    "                            record_key = (record[\"TrackId\"],)\n",
    "                        elif subfolder_name == \"playlist_track\":\n",
    "                            record_key = (record[\"PlaylistId\"], record[\"TrackId\"])\n",
    "                        elif subfolder_name == \"track_facts\":\n",
    "                            record_key = (record[\"genre\"], record[\"minutes\"], record[\"price\"])\n",
    "                        elif subfolder_name == \"orders\":\n",
    "                            record_key = (\n",
    "                                record[\"invoice_date\"],\n",
    "                                record[\"UnitPrice\"],\n",
    "                                record[\"genre\"],\n",
    "                                record[\"media_type\"],\n",
    "                                record[\"minutes\"],\n",
    "                            )\n",
    "                        else:\n",
    "                            record_key = tuple(sorted(record.items()))\n",
    "\n",
    "                        if record_key in seen_records:\n",
    "                            continue\n",
    "                        seen_records.add(record_key)\n",
    "\n",
    "                        gz_file.write(json.dumps(record) + \"\\n\")\n",
    "                        processed_count += 1\n",
    "\n",
    "                    except json.JSONDecodeError:\n",
    "                        log.write(f\" Skipping malformed line in {file_to_process}\\n\")\n",
    "\n",
    "    print(f\"Processed {processed_count} unique records from {subfolder_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "graphic-software",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Processing only: tracks_0.csv\n",
      "Processed 3503 unique records from tracks\n",
      " Processing only: playlist_track_0.csv\n",
      "Processed 8715 unique records from playlist_track\n",
      " Processing only: orders_0.csv\n",
      "Processed 2142 unique records from orders\n",
      " Processing only: track_facts_0.csv\n",
      "Processed 782 unique records from track_facts\n"
     ]
    }
   ],
   "source": [
    "for subfolder in RAW_DIR.iterdir():\n",
    "    if subfolder.is_dir():\n",
    "        process_subfolder_fixed(subfolder, PROCESSED_DIR, ERROR_LOG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "defensive-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_unique_records_per_dataset(raw_dir):\n",
    "    \"\"\"\n",
    "    Counts unique records for each dataset before processing.\n",
    "    \"\"\"\n",
    "    dataset_counts = {}\n",
    "    \n",
    "    for subfolder in raw_dir.iterdir():\n",
    "        if subfolder.is_dir():\n",
    "            files = sorted(subfolder.iterdir())\n",
    "            file_to_process = next((f for f in files if f.suffix == \".csv\"), None)  # Prefer CSV\n",
    "            if file_to_process is None:\n",
    "                file_to_process = next((f for f in files if f.suffix == \".json\"), None)  # Fallback to JSON\n",
    "\n",
    "            if not file_to_process:\n",
    "                continue  # Skip empty subfolders\n",
    "\n",
    "            seen_records = set()\n",
    "            print(f\" Checking unique records in {file_to_process.name}\")\n",
    "\n",
    "            if file_to_process.suffix == \".csv\":\n",
    "                for chunk in pd.read_csv(file_to_process, chunksize=1000):\n",
    "                    for record in chunk.to_dict(orient=\"records\"):\n",
    "                        record = {k.split(\".\")[-1]: v for k, v in record.items()}  # Normalize column names\n",
    "                        seen_records.add(tuple(record.items()))\n",
    "\n",
    "            elif file_to_process.suffix == \".json\":\n",
    "                with open(file_to_process, \"r\") as json_file:\n",
    "                    for line in json_file:\n",
    "                        record = json.loads(line.strip())\n",
    "                        record = {k.split(\".\")[-1]: v for k, v in record.items()}  # Normalize column names\n",
    "                        seen_records.add(tuple(record.items()))\n",
    "\n",
    "            dataset_counts[subfolder.name] = len(seen_records)\n",
    "\n",
    "    total_unique_records = sum(dataset_counts.values())\n",
    "    return total_unique_records, dataset_counts\n",
    "\n",
    "\n",
    "def count_output_records(processed_dir):\n",
    "    \"\"\"\n",
    "    Counts unique records in processed .json.gz files.\n",
    "    \"\"\"\n",
    "    output_counts = defaultdict(int)\n",
    "\n",
    "    for file in processed_dir.glob(\"*.json.gz\"):\n",
    "        seen_records = set()\n",
    "        with gzip.open(file, \"rt\") as gz_file:\n",
    "            for line in gz_file:\n",
    "                record = json.loads(line.strip())\n",
    "                record = {k.split(\".\")[-1]: v for k, v in record.items()}  # Normalize column names\n",
    "                seen_records.add(tuple(record.items()))\n",
    "        output_counts[file.name] = len(seen_records)\n",
    "\n",
    "    total_output_records = sum(output_counts.values())\n",
    "    return total_output_records, output_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "enhanced-sudan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Checking unique records in tracks_0.csv\n",
      " Checking unique records in playlist_track_0.csv\n",
      " Checking unique records in orders_0.csv\n",
      " Checking unique records in track_facts_0.csv\n",
      "> Unique Input Records (before processing): 15142\n",
      "> Total Output Records (after processing): 15142\n",
      "> Per Dataset Input Counts: {'tracks': 3503, 'playlist_track': 8715, 'orders': 2142, 'track_facts': 782}\n",
      "> Per Processed File Counts: defaultdict(<class 'int'>, {'playlist_track.json.gz': 8715, 'orders.json.gz': 2142, 'track_facts.json.gz': 782, 'tracks.json.gz': 3503})\n",
      ">> Records Matched: True\n"
     ]
    }
   ],
   "source": [
    "unique_input_count, dataset_counts = count_unique_records_per_dataset(RAW_DIR)\n",
    "output_total, output_details = count_output_records(PROCESSED_DIR)\n",
    "\n",
    "print(f\"> Unique Input Records (before processing): {unique_input_count}\")\n",
    "print(f\"> Total Output Records (after processing): {output_total}\")\n",
    "print(f\"> Per Dataset Input Counts: {dataset_counts}\")\n",
    "print(f\"> Per Processed File Counts: {output_details}\")\n",
    "print(f\">> Records Matched: {unique_input_count == output_total}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-reproduction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
